{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras_applications import inception_v3\n",
    "from keras_preprocessing.image import img_to_array, load_img\n",
    "\n",
    "img_path = \"datasets/flickr8k/Flickr8k_Dataset/667626_18933d713e.jpg\"\n",
    "img = load_img(img_path, target_size=(299, 299))\n",
    "img_array = img_to_array(img)\n",
    "img_array = inception_v3.preprocess_input(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.text import Tokenizer\n",
    "\n",
    "all_sentences = [\"A women lay in side of pool.\", \"A young girl is lying in the sand , while ocean water is surrounding her.\", \"Girl wearing a bikini lying on her back in a shallow pool of clear blue water.\"]\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_sentences)\n",
    "\n",
    "sentences_to_encode = all_sentences[:2]\n",
    "encoded_sentences = tokenizer.texts_to_sequences(sentences_to_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_applications.inception_v3 import InceptionV3\n",
    "from keras.layers import BatchNormalization, Dense, RepeatVector\n",
    "\n",
    "image_model = InceptionV3(include_top=False, weights='imagenet', pooling='avg')\n",
    "\n",
    "for layer in image_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "embedding_size = 300\n",
    "dense_input = BatchNormalization(axis=-1)(image_model.output)\n",
    "image_dense = Dense(units=embedding_size)(dense_input)\n",
    "\n",
    "image_embedding = RepeatVector(1)(image_dense)\n",
    "image_input = image_model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Input\n",
    "\n",
    "vocab_size = 2536\n",
    "embedding_size = 300\n",
    "\n",
    "sentence_input = Input(shape=[None])\n",
    "word_embedding = Embedding(input_dim=vocab_size,\n",
    "                           output_dim=embedding_size\n",
    "                           )(sentence_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def categorical_crossentropy_from_logits(y_true, y_pred):\n",
    "  y_true = y_true[:, :-1, :]  # Discard the last timestep\n",
    "  y_pred = y_pred[:, :-1, :]  # Discard the last timestep\n",
    "  loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_true,\n",
    "                                                 logits=y_pred)\n",
    "  return loss\n",
    "\n",
    "def categorical_accuracy_with_variable_timestep(y_true, y_pred):\n",
    "  y_true = y_true[:, :-1, :]  # Discard the last timestep\n",
    "  y_pred = y_pred[:, :-1, :]  # Discard the last timestep\n",
    "\n",
    "  # Flatten the timestep dimension\n",
    "  shape = tf.shape(y_true)\n",
    "  y_true = tf.reshape(y_true, [-1, shape[-1]])\n",
    "  y_pred = tf.reshape(y_pred, [-1, shape[-1]])\n",
    "\n",
    "  # Discard rows that are all zeros as they represent padding words.\n",
    "  is_zero_y_true = tf.equal(y_true, 0)\n",
    "  is_zero_row_y_true = tf.reduce_all(is_zero_y_true, axis=-1)\n",
    "  y_true = tf.boolean_mask(y_true, ~is_zero_row_y_true)\n",
    "  y_pred = tf.boolean_mask(y_pred, ~is_zero_row_y_true)\n",
    "\n",
    "  accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_true, axis=1),\n",
    "                                              tf.argmax(y_pred, axis=1)),\n",
    "                                    dtype=tf.float32))\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-6fc787e89e6f>:7: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import (BatchNormalization, Concatenate, Dense, LSTM,TimeDistributed)\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "sequence_input = Concatenate(axis=1)([image_embedding, word_embedding])\n",
    "\n",
    "learning_rate = 0.00051\n",
    "lstm_output_size = 300\n",
    "vocab_size = 2536\n",
    "lstm_layers = 3\n",
    "dropout_rate = 0.22\n",
    "input_ = sequence_input\n",
    "\n",
    "for _ in range(lstm_layers):\n",
    "  input_ = BatchNormalization(axis=-1)(input_)\n",
    "  lstm_out = LSTM(units=lstm_output_size,\n",
    "                  return_sequences=True,\n",
    "                  dropout=dropout_rate,\n",
    "                  recurrent_dropout=dropout_rate)(input_)\n",
    "  input_ = lstm_out\n",
    "sequence_output = TimeDistributed(Dense(units=vocab_size))(lstm_out)\n",
    "\n",
    "model = Model(inputs=[image_input, sentence_input],\n",
    "              outputs=sequence_output)\n",
    "model.compile(optimizer=Adam(lr=learning_rate),\n",
    "              loss=categorical_crossentropy_from_logits,\n",
    "              metrics=[categorical_accuracy_with_variable_timestep])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataset_providers'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-423d0d121af3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset_providers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatasetProvider\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset_provider\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetProvider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m model.fit_generator(generator=dataset_provider.training_set(),\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataset_providers'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from .dataset_providers import DatasetProvider\n",
    "\n",
    "dataset_provider = DatasetProvider()\n",
    "\n",
    "model.fit_generator(generator=dataset_provider.training_set(),\n",
    "                    steps_per_epoch=dataset_provider.training_steps,\n",
    "                    epochs=1,\n",
    "                    validation_data=dataset_provider.validation_set(),\n",
    "                    validation_steps=dataset_provider.validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
